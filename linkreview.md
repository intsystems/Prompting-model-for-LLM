# LinkReview

- Here we have collect info about all the works that may be useful for writing our paper

> [!NOTE]
> This review table will be updated, so it is not a final version

| Title | Year | Authors | Paper | Code | Summary |
| :--- | ---: | :--- | :--- | :--- | :--- |
| LLM-Informed Discrete Prompt Optimization | 2024 | Zeeshan Memon | [paper](https://openreview.net/pdf?id=d0jQuZe6k0) | [GitHub](https://www.youtube.com/watch?v=MuRa3tlyzq8) | Это работа является ключевой для нашего исследование. В ней авторы разбивают обучение prompt'ов на 2 части: первая часть общая для всех LLM, в ней подразумевается наличие датасета с простыми prompt'ами и улучшенными, чтобы легкий Backbone затюнился лучше сэмплировать хорошие подсказки. Вторая часть обучения проводится для каждой LLM отдельно, в ней MLP и HEAD дообучаются для конкретной LLM для генерации ключевых слов. Авторы утверждают, что у каждой LLM есть набор ключевых слов, использование которых может существенно улучшить prompt'ы и, соответственно, ответы LLM. |
| Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks | 2017 | Chelsea Finn | [paper](https://proceedings.mlr.press/v70/finn17a/finn17a.pdf) | [GitHub]() | [Simple explaination](https://interactive-maml.github.io/maml.html) На маленьком датасете под конкретную задачу эмпиристически известно, что тюнится плохо. Берут задачи, схожие с решаемой, считаем что это из одного распределения. Сэмплируем батч из каждой таски, делаем шаг градиентного спуска для каждой задачи, и получаем обновленные веса модели (или её изменяемой части) для каждой задачи, и потом делаем шаг градиентного спуска, состоящим из градиентов по каждой таске, но в этом градиенте обновленные веса для каждой таске. То есть получается более обобщенный градиент, так как градиент берется по всем задачам и плюс к этому градиент уже подтюнился под каждую из задач. |
| Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery | 2023 | Yuxin Wen, Neel Jain, John Kirchenbauer | [paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/a00548031e4647b13042c97c922fadf1-Paper-Conference.pdf) | [GitHub](https://github.com/YuxinWenRick/hard-prompts-made-easy) | Learning hard prompts for image generation using continuous optimization. The scheme builds on existing gradient reprojection schemes for optimizing text. Берут непрерывные промпты и на каждом шагу проецируют на дискретное пространство, затем оптимизируют градиентым спуском как непрерывные. |
| How Hard Can It Prompt? Adventures in Cross-model Prompt Transferability | 2024 | Lola Solovyeva | [paper](https://essay.utwente.nl/103206/1/Solovyeva_MA_EEMCS.pdf) | [GitHub]() | Discretizing soft prompts by leveraging cosine similarity between the embeddings of soft and hard tokens. Algorithm designed to identify a set of hard tokens using gradients obtained through the tuning of soft prompts. Testing the transferability of the derived hard prompts between different models. Написано примерно то же, что и в предыдущей статье, но в виде более подробной книжки с усложнением алгоритма из статьи выше. |
| Dynamic Prompting: A Unified Framework for Prompt Tuning | 2023 | Xianjun Yang | [paper](https://arxiv.org/pdf/2303.02909) | [GitHub](https://interactive-maml.github.io/maml.html) | TODO |
| Automatic Prompt Optimization with “Gradient Descent” and Beam Search | 2023 | R Pryzant, D Iter | [paper](https://aclanthology.org/2023.emnlp-main.494.pdf) | [GitHub]() | TODO |
| Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners | 2022 | Ningyu Zhang Luoqiu Li | [paper](https://arxiv.org/pdf/2108.13161) | [GitHub]() | TODO |
| LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples | 2022 | Jia-Yu Yao Kun-Peng Ning | [paper](https://arxiv.org/pdf/2310.01469) | https://github.com/PKU-YuanGroup/Hallucination-Attack | В статье рассматриваются галлюцинации. Показываается, что галлюцинации возникают не из-за неправильного обучения модели, а заложены в архитектуре трансформера, то есть для любой LLM можно сгенерировать prompt, на котором модель будет галлюцинировать. Далее в статье рассматривают способы дообучения для больб с галлюцинациями. Рассматривается генерация propmt'ов, на который модели обычно галлюцинируют (2 типа: когда плохо заложен смысл и когда подается бред на вход). Создается вспомогательная модель (Hallucination-Attack) модель, которая генерирует такие propmpt'ы и показывается, как можно дообучать модель для борьбы с этим. |
| Soft prompting might be a bug, not a feature | 2023 | Luke Bailey | [paper](https://openreview.net/forum?id=MHWDdMEJ5s#all) | [GitHub]() | Потенциальная уязвимость soft prompts. Почему лучше работать с hard prompt. |
| Prefix-tuning: Optimizing continuous prompts for generation | 2021 | Xiang Lisa Li and Percy Liang | [paper](https://aclanthology.org/2021.acl-long.353) | [GitHub]() | Fine-tune не для всей предобученной языковой модели, а для её промптов. |
| Late prompt tuning: A late prompt could be better than many prompts | 2022 | Xiangyang Liu | [paper](https://api.semanticscholar.org/CorpusID:253018816) | [GitHub]() | Умная вставка промптов не в начало обучения, а в середину. |
| The Power of Scale for Parameter-Efficient Prompt Tuning | 2021 | Brian Lester, Rami Al-Rfou, Noah Constant | [paper](https://aclanthology.org/2021.emnlp-main.243/) | [GitHub]() | In this paper, authors propose prompt tuning as a simplification for adapting language models. For this it is necessary to freeze the entire pre-trained model and only allow an additional k tunable tokens per downstream task to be prepended to the input text. This “soft prompt” is trained end-to-end and can condense the signal from a full labeled dataset, allowing method to outperform few-shot prompts and close the quality gap with model tuning the same time, since a single pre-trained model is recycled for all downstream tasks, the efficient serving benefits of frozen models are retained. Authors show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” |
| RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning | 2022 | Mingkai Deng | [paper](https://arxiv.org/pdf/2205.12548) | [GitHub](https://github.com/mingkaid/rl-prompt) | Предлагают использовать RL для дискретных промптов, в качестве награды используют z-score. Сэмплируют батч промптов для входа x, считают награду для них, потом считают z-score, вышло хуже чем тюнить, но лучше чем остальные подходы и плюс к этому почти без вычислительных ресурсов. |
| Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference | 2021 | Timo Schick, Hinrich Schutze | [paper](https://arxiv.org/pdf/2001.07676) | [GitHub](https://github.com/timoschick/pet) | Собрали датасет с лучшими промптами для llm-ок |
