{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e331f1-51c1-47c0-9795-01d68e2ffcd9",
   "metadata": {},
   "source": [
    "### Clean dataset from trash, specific symbols and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e39f52-a3df-4456-ac6f-e0566b4669e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./cleared_experiment_data.csv\")\n",
    "\n",
    "def clean_string(s):\n",
    "    phrases = [\n",
    "        'Upgrade prompt', 'Refine the prompt to', 'Upgraded Prompt', \n",
    "        'Prompt to improve', 'Enhance the following prompt', 'Enhanced prompt', \n",
    "        'Prompt to improve', 'Rewritten prompt', 'Revise the prompt',\n",
    "        'Refine prompt', 'Enhanced Prompt', 'Enhance the prompt', \n",
    "        'Enhance this prompt', 'Improved Prompt', 'to improve',\n",
    "        'Reframed prompt', 'Reframe the prompt to', 'Revised prompt', \n",
    "        'Revise prompt', 'Streamline prompt', 'Here\\'s an improved prompt',\n",
    "        'Modify prompt', 'Refined prompt', 'Refine the prompt',\n",
    "        'Prompt',         \n",
    "    ]\n",
    "    pattern = r'^\\s*(?:' + '|'.join(re.escape(phrase) for phrase in phrases) + r'|\\*\\*[^*]+\\*\\*)\\s*'\n",
    "    \n",
    "    s = re.sub(pattern, '', s)\n",
    "    s = re.sub(r\"^[,\\s'\\-\\\":]+|[,\\s'\\-\\\"]+$\", '', s)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100cf718-b954-494f-bce7-51c2737e476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'Mistral_prompts']    = 'Code a program to determine the area of any given triangle using a specific formula (e.g., base times height divided by 2).'\n",
    "df.loc[15, 'Mistral_prompts']   = 'Detail the step-by-step procedure for debugging C++ code, highlighting essential techniques and popular tools employed in the process.'\n",
    "df.loc[16, 'Mistral_prompts']   = 'Best practices for effective collaboration on coding projects.'\n",
    "df.loc[22, 'Mistral_prompts']   = 'I aim to develop a gRPC server and client application in Python. I need guidance on the steps and best practices for setting up the project, defining the service and messages in the .proto file, generating the Python code, implementing the server, and finally, creating the client to communicate with the server. I would appreciate advice on error handling, testing, and deploying the gRPC application'\n",
    "df.loc[26, 'Mistral_prompts']   = \"What are the steps to connect and interact with a PostgreSQL database using Python?\"\n",
    "df.loc[36, 'Mistral_prompts']   = \"Could you explain the basic steps to utilize Git, a version control system, in a simplified manner?\"\n",
    "df.loc[78, 'Mistral_prompts']   = \"Write a C++ code snippet that calculates the sum of even numbers in an array.\"\n",
    "df.loc[80, 'Mistral_prompts']   = \"Can you explain the role of sugar in the body and how it relates to diabetes?\"\n",
    "df.loc[96, 'Mistral_prompts']   = \"Exploring the Influence of Cloud Computing on Data Management\"\n",
    "df.loc[128, 'Mistral_prompts']  = \"How does 'Crime and Punishment' explore the complexities of the human mind?\"\n",
    "df.loc[133, 'Mistral_prompts']  = \"What is the underlying message or theme conveyed in 'Fahrenheit 451'?\"\n",
    "df.loc[155, 'Mistral_prompts']  = \"GPS Technology: Could you explain its functionality?\"\n",
    "df.loc[160, 'Mistral_prompts']  = \"Innovative methods or techniques for preserving food, suggestions?\"\n",
    "df.loc[174, 'Mistral_prompts']  = \"Can you describe the process of brewing beer and the key steps involved?\"\n",
    "df.loc[199, 'Mistral_prompts']  = \"Explain the impact of European colonization on Africa.\"\n",
    "df.loc[200, 'Mistral_prompts']  = \"Explain the concept of relativity theory in layman\\'s terms.\"\n",
    "df.loc[221, 'Mistral_prompts']  = \"Explain the benefits of traveling via buses and trains.\"\n",
    "df.loc[225, 'Mistral_prompts']  = \"What are the psychological and emotional effects of being alone for extended periods of time?\"\n",
    "df.loc[258, 'Mistral_prompts']  = \"Narrate the unnerving details and hair-raising ambiance that encompass a haunted house, focusing on its terrifying elements and ominous aura.\"\n",
    "df.loc[440, 'Mistral_prompts']  = \"Craft a compelling narrative about a heist that goes terribly wrong, including the planning, execution, and aftermath.\"\n",
    "df.loc[493, 'Mistral_prompts']  = \"Could you explain the process and stages of the water cycle?\"\n",
    "df.loc[541, 'Mistral_prompts']  = \"What were the key incidents that triggered and concluded World War I?\"\n",
    "df.loc[554, 'Mistral_prompts']  = \"Discuss the theme of war as depicted in Ernest Hemingway\\'s \\'A Farewell to Arms\\'.\"\n",
    "df.loc[575, 'Mistral_prompts']  = \"Why does the Battle of Gettysburg hold such historical significance?\"\n",
    "df.loc[584, 'Mistral_prompts']  = \"Craft a suspenseful narrative set in the golden age of Hollywood, filled with intrigue and enigma.\"\n",
    "df.loc[707, 'Mistral_prompts']  = \"How is gene editing revolutionizing the field of medicine?\"\n",
    "df.loc[717, 'Mistral_prompts']  = \"Explain human development, including its key dimensions and stages.\"\n",
    "df.loc[726, 'Mistral_prompts']  = \"Discuss the incorporation of irony in the narrative of \\'Gulliver\\'s Travels\\'.\"\n",
    "df.loc[729, 'Mistral_prompts']  = \"How does \\'The Catcher in the Rye\\' depict human nature and identity?\"\n",
    "df.loc[802, 'Mistral_prompts']  = \"How does \\\"Heart of Darkness\\\" explore the themes of colonialism and identity?\"\n",
    "df.loc[871, 'Mistral_prompts']  = \"Justify the Preservation of Rainforests.\"\n",
    "df.loc[941, 'Mistral_prompts']  = \"How did the French Revolution impact Europe?\"\n",
    "df.loc[975, 'Mistral_prompts']  = \"Can you explain the concept of biodiversity?\"\n",
    "df.loc[1080, 'Mistral_prompts'] = \"How has the Internet transformed communication practices?\"\n",
    "df.loc[1176, 'Mistral_prompts'] = \"What are the principles and significance of energy conservation?\"\n",
    "df.loc[1184, 'Mistral_prompts'] = \"How does Shakespeare portray the nature of love in \\\"Romeo and Juliet\\\"?\"\n",
    "df.loc[1193, 'Mistral_prompts'] = \"Discuss the ethical considerations surrounding gene therapy and its applications in healthcare.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d49a7d-f38d-4e2d-a635-62f1d44c301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[38, 'gpt-4o-mini_prompts']  = \"Can you guide me through the steps to create a mobile app for tracking fitness goals?\"\n",
    "df.loc[51, 'gpt-4o-mini_prompts']  = \"Please provide a C++ function that sorts an array using the quicksort algorithm, including example usage.\"\n",
    "df.loc[65, 'gpt-4o-mini_prompts']  = \"Provide a JavaScript function to validate phone number formats, including examples for different formats (e.g., international, local).\"\n",
    "df.loc[72, 'gpt-4o-mini_prompts']  = \"Can you provide a Python function to calculate the GCD of two integers using the Euclidean algorithm?\"\n",
    "df.loc[112, 'gpt-4o-mini_prompts'] = \"Explain the historical significance of the Silk Road and its impact on trade and culture.\"\n",
    "df.loc[332, 'gpt-4o-mini_prompts'] = \"Create a humorous story about a treasure hunt that goes hilariously wrong, featuring unexpected obstacles and quirky characters.\"\n",
    "df.loc[399, 'gpt-4o-mini_prompts'] = \"Can you summarize the key principles of quantum mechanics and their implications?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956a95bc-1d05-464a-a981-14c7d8653cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_idx = []\n",
    "for column in df.columns:\n",
    "    for idx in range(len(df[column])):\n",
    "        clean_prompt = clean_string(df[column][idx])\n",
    "        if 'prompt' in clean_prompt:\n",
    "            print(idx, df[column][idx])\n",
    "            bad_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52d89fb-c3cc-45fc-b650-946eee3e810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b0a9c8-7d55-4c2b-9f2a-a1cac979e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = {\n",
    "    'bad_prompt': [],\n",
    "    'improved_prompt': [],\n",
    "    'mistral_prompt': [],\n",
    "    'gpt_prompt': [],\n",
    "    'messages_improved': [],\n",
    "    'messages_mistral': [],\n",
    "    'messages_gpt': [],\n",
    "    'target': []\n",
    "}\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    bad_prompt     = df['bad_prompts'][idx]\n",
    "    good_prompt    = clean_string(df['improved_prompts'][idx])\n",
    "    prompt_gpt     = clean_string(df['gpt-4o-mini_prompts'][idx])\n",
    "    prompt_mistral = clean_string(df['Mistral_prompts'][idx])\n",
    "    \n",
    "    #input_instr = f\"<|im_start|>user\\n Improve this prompt - '{bad_prompt}'.\\n<|im_start|>assistant\"\n",
    "    #out_good    = f\"Improved prompt - {good_prompt}<|im_end|>\"\n",
    "    #out_mistral = f\"Improved prompt - {prompt_mistral}<|im_end|>\"\n",
    "    #out_gpt     = f\"Improved prompt - {prompt_gpt}<|im_end|>\"\n",
    "    \n",
    "    input_instr = f\"Instruct: Improve this prompt - '{bad_prompt}'.\\nOutput:\"\n",
    "    out_good    = f\"Output: Improved prompt - {good_prompt}\"\n",
    "    out_mistral = f\"Output: Improved prompt - {prompt_mistral}\"\n",
    "    out_gpt     = f\"Output: Improved prompt - {prompt_gpt}\"\n",
    "\n",
    "    #input_instr = f\"Improve this prompt - '{bad_prompt}'.\"\n",
    "    #out_good    = f\"Improved prompt - {good_prompt}\"\n",
    "    #out_mistral = f\"Improved prompt - {prompt_mistral}\"\n",
    "    #out_gpt     = f\"Improved prompt - {prompt_gpt}\"\n",
    "    #\"Instruct: <prompt>\\nOutput:\"\n",
    "    new_dataset['target'].append(f\"Instruct: Improve this prompt - '{bad_prompt}'.\\nOutput: Improved prompt - {good_prompt}\")\n",
    "    \n",
    "    new_dataset['bad_prompt'].append(input_instr)\n",
    "    new_dataset['improved_prompt'].append(out_good)\n",
    "    new_dataset['mistral_prompt'].append(out_mistral)\n",
    "    new_dataset['gpt_prompt'].append(out_gpt)\n",
    "\n",
    "    new_dataset['messages_improved'].append([\n",
    "        {\"role\":\"user\", \"content\": f\"Improve this prompt - '{bad_prompt}'\"},\n",
    "        {\"role\":\"assistant\", \"content\": f\"Improved prompt - '{good_prompt}'\"},\n",
    "    ])\n",
    "    new_dataset['messages_mistral'].append([\n",
    "        {\"role\":\"user\", \"content\": f\"Improve this prompt - '{bad_prompt}'\"},\n",
    "        {\"role\":\"assistant\", \"content\": f\"Improved prompt - '{out_mistral}'\"},\n",
    "    ])\n",
    "    new_dataset['messages_gpt'].append([\n",
    "        {\"role\":\"user\", \"content\": f\"Improve this prompt - '{bad_prompt}'\"},\n",
    "        {\"role\":\"assistant\", \"content\": f\"Improved prompt - '{out_gpt}'\"},\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7947dca6-2526-4a16-9e60-bd65051e01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_dataset)\n",
    "new_df.head()\n",
    "new_df.to_csv('prepared_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84bcf1-8463-4af8-bfbc-a4bd31295362",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc18cde3-a1b5-4c57-bb56-b361442d1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, cast_mixed_precision_params\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8806a6fd-f300-4058-922e-e5e70cc30b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bbf84f0a844a7e85733c68c800a008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./prepared_data.csv\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13fea19-4a96-423d-863c-b8775270569f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f437c7da464d7bb1f94e18772e5383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    padding_side='left',\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=False,\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"half\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = LoraConfig(\n",
    "    r=16,  \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"k_proj\", \"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model = get_peft_model(model, config)\n",
    "#cast_mixed_precision_params(model, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59e33eb4-2596-4076-b8e1-15c30f0a5ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e7efd81acc44369674cdbc27951ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c04b8ba6b8a544c794748a2d81c59283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokeniza_data(example):\n",
    "    #tokenized_inputs = tokenizer(example[\"bad_prompt\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    #tokenized_inputs[\"labels\"] = tokenizer(example[\"improved_prompt\"], padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"]\n",
    "    tokenized_inputs = tokenizer(example[\"target\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs['input_ids'].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokeniza_data, batched=True, remove_columns=['target', 'bad_prompt', 'improved_prompt', 'mistral_prompt', 'gpt_prompt', 'messages_improved', 'messages_mistral', 'messages_gpt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "250e1718-f4eb-4a5b-800a-08aea23122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.dataset['input_ids'][idx]), torch.tensor(self.dataset['attention_mask'][idx]), torch.tensor(self.dataset['labels'][idx]), \n",
    "\n",
    "train_dataset = Seq2SeqDataset(tokenized_dataset['train'])\n",
    "test_dataset = Seq2SeqDataset(tokenized_dataset['test'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d8f402-4227-45eb-9de3-521a22135f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "\n",
    "    for inputs, mask, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=inputs, labels=labels, attention_mask=mask)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(total_loss)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, mask, labels in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids=inputs, labels=labels, attention_mask=mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67631cfa-52d1-4e29-9351-53f9d3f141c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import get_scheduler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8595b86e-6613-49fa-a6dc-57a100b88c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkreininmv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kreinin.mv/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ssd/kreinin.mv/university/prompt/wandb/run-20241028_222259-6zthbrbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kreininmv/llm-prompt/runs/6zthbrbg' target=\"_blank\">grateful-night-7</a></strong> to <a href='https://wandb.ai/kreininmv/llm-prompt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kreininmv/llm-prompt' target=\"_blank\">https://wandb.ai/kreininmv/llm-prompt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kreininmv/llm-prompt/runs/6zthbrbg' target=\"_blank\">https://wandb.ai/kreininmv/llm-prompt/runs/6zthbrbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key='84d6a92704bf4bf19d2ecc87a55eea5ce77a8725')\n",
    "wandb_config = {\n",
    "    'lora_config': config,\n",
    "    'lr': 2e-5,\n",
    "    'wd': 0.01,\n",
    "    'optimizer': 'AdamW',\n",
    "    'num_epochs': 1000\n",
    "}\n",
    "run = wandb.init(project='llm-prompt', config=wandb_config)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2acb1d3-41df-45f6-86b2-9860e970d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/137 [00:00<?, ?it/s]/home/kreinin.mv/env/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Training:  10%|█         | 14/137 [01:07<09:39,  4.71s/it]"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=wandb_config['lr'], weight_decay=wandb_config['wd'])\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(wandb_config['num_epochs']):\n",
    "    print(f\"Epoch {epoch + 1}/{wandb_config['num_epochs']}\")\n",
    "\n",
    "    # Обучение\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    eval_loss = evaluate(model, test_loader, device)\n",
    "    \n",
    "    wandb.log({\n",
    "        f\"Training loss\": train_loss,\n",
    "        f\"Validation loss\": eval_loss,\n",
    "    })\n",
    "\n",
    "    # Оценка\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation loss: {eval_loss:.4f}\")\n",
    "\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Best model saved with loss: {best_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
